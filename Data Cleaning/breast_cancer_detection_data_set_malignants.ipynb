{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributors: Hyeeun Hughes, Arnold Schultz, Mauvonte Roberts, Ryan Grimsley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breast Cancer Data (122 kB): https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": Number of instances: 569\n",
    "\n",
    ": Number of attributes: 32\n",
    "\n",
    ": Attribute information:\n",
    "   \n",
    "       1) ID number\n",
    "\n",
    "       2) Diagnosis (M = malignant, B = benign)\n",
    "       \n",
    "       3-32) Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "          a) radius (mean of distances from center to points on the perimeter)\n",
    "\n",
    "          b) texture (standard deviation of gray-scale values)\n",
    "\n",
    "          c) perimeter\n",
    "\n",
    "          d) area\n",
    "\n",
    "          e) smoothness (local variation in radius lengths)\n",
    "\n",
    "          f) compactness (perimeter^2 / area - 1.0)\n",
    "\n",
    "          g) concavity (severity of concave portions of the contour)\n",
    "\n",
    "          h) concave points (number of concave portions of the contour)\n",
    "\n",
    "          i) symmetry\n",
    "\n",
    "          j) fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "\n",
    ": Missing attribute values: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import and read the breast-cancer.data.csv.\n",
    "df = pd.read_csv(\"../Resources/data.csv\")\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of mean, se, and worst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean: The average\n",
    "\n",
    "* Standard Error: The standard error of the mean\n",
    "\n",
    "* Worst: The mean of the three largest values(features were computed for each image, resulting in 30 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-naming columns\n",
    "# df.rename(columns={'radius_worst': 'radius_largest', \n",
    "#                    'texture_worst': 'texture_largest',\n",
    "#                    'perimeter_worst': 'perimeter_largest',\n",
    "#                    'area_worst': 'area_largest',\n",
    "#                    'smoothness_worst': 'smoothness_largest',\n",
    "#                    'compactness_worst': 'compactness_largest',\n",
    "#                    'concavity_worst': 'concavity_largest',\n",
    "#                    'concave_points_worst': 'concave_largest',\n",
    "#                    'symmetry_worst': 'symmetry_largest',\n",
    "#                    'fractal_dimension_worst': 'cfractal_dimension_largest',\n",
    "#                      }, inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key challenge against its detection is how to classify tumors into malignant (cancerous) or benign(non-cancerous). We'll be completing the analysis of classifying these tumors using machine learning (with SVMs) and the Breast Cancer Wisconsin (Diagnostic) Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'diagnosis' value count\n",
    "df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find null values\n",
    "for column in df.columns:\n",
    "    print(f\"Column {column} has {df[column].isnull().sum()} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate entries\n",
    "print(f\"Duplicate entries: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at 'diagnosis' value counts for binning\n",
    "val_counts = df['diagnosis'].value_counts()\n",
    "val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cutoff value and create a list of diagnosis to be replaced\n",
    "# use the variable name `diagnosis_to_replace`\n",
    "\n",
    "# Transform diagnosis\n",
    "def diagnosis_to_replace(diagnosis):\n",
    "    if diagnosis == \"M\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].apply(diagnosis_to_replace)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cutoff value and create a list of diagnosis to be replaced\n",
    "# use the variable name `diagnosis_to_replace`\n",
    "diagnosis_to_replace = list(val_counts [val_counts == 1].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in diagnosis_to_replace:\n",
    "    df['diagnosis'] = df['diagnosis'].replace(app,\"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure binning was successful\n",
    "df[\"diagnosis\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at radius_worst value counts for binning\n",
    "# radius_worst_value_counts = df['radius_worst'].value_counts()\n",
    "# radius_worst_value_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at texture_worst value counts for binning\n",
    "# texture_worst_value_counts = df['texture_worst'].value_counts()\n",
    "# texture_worst_value_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at area_worst value counts for binning\n",
    "# area_worst_value_counts = df['area_worst'].value_counts()\n",
    "# area_worst_value_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at perimeter_worst value counts for binning\n",
    "# perimeter_worst_value_counts = df['perimeter_worst'].value_counts()\n",
    "# perimeter_worst_value_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"diagnosis\" column from the dataset\n",
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "X = df.drop(columns=['diagnosis'])\n",
    "y = df['diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Instantiate KNN model and make predictions\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred = knn.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Assess the accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Apply Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crestr a new dataframe for t-sne\n",
    "df2 = df.drop(['diagnosis'], axis = 1)\n",
    "labels = df['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialze t-sne model\n",
    "tsne = TSNE(learning_rate=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimesins\n",
    "tsne_features = tsne.fit_transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has 2 columns\n",
    "tsne_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ro plot the dataset\n",
    "# The first column of transformed features\n",
    "df2['x']=tsne_features[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['y']=tsne_features[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters\n",
    "plt.scatter(df2['x'], df2['y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters with color\n",
    "plt.scatter(df2['x'], df2['y'], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarized data with StandarsScaler\n",
    "df_scaled = StandardScaler().fit_transform(df)\n",
    "print(df_scaled[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA to reduce dimensions from 32 to 2\n",
    "# Initialize PCA model\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tow principal components for the myopia data\n",
    "df_pca = pca.fit_transform(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform PCA data to a DataFrme\n",
    "df_pca = pd.DataFrame(\n",
    "    data=df_pca, columns=[\"principal component 1\", \"principal component 2\"])\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the explained variance\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the data to X and y\n",
    "# Note: Sklearn requires a two-dimensional array of values\n",
    "# so we use reshape() to create this\n",
    "\n",
    "# X = df[['id', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "#        'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "#        'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "#        'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "#        'compactness_se', 'concavity_se', 'concave_points_se', 'symmetry_se',\n",
    "#        'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "#        'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "#        'compactness_worst', 'concavity_worst', 'concave_points_worst',\n",
    "#        'symmetry_worst', 'fractal_dimension_worst']]\n",
    "# y = df['diagnosis']\n",
    "\n",
    "# print(\"Shape: \", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "X = df.drop(columns=['diagnosis'])\n",
    "y = df['diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model print the model score\n",
    "classifier = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "classifier = RandomForestClassifier(random_state=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
