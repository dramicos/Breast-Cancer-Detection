{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of instances: 569\n",
    "\n",
    "* Number of attributes: 32\n",
    "\n",
    "* Attribute information:\n",
    "   \n",
    "   1) ID number\n",
    "\n",
    "   2) Diagnosis (M = malignant, B = benign)\n",
    "3-32)\n",
    "\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "  a) radius (mean of distances from center to points on the perimeter)\n",
    "\n",
    "  b) texture (standard deviation of gray-scale values)\n",
    "\n",
    "  c) perimeter\n",
    "\n",
    "  d) area\n",
    "\n",
    "  e) smoothness (local variation in radius lengths)\n",
    "\n",
    "  f) compactness (perimeter^2 / area - 1.0)\n",
    "\n",
    "  g) concavity (severity of concave portions of the contour)\n",
    "\n",
    "  h) concave points (number of concave portions of the contour)\n",
    "\n",
    "  i) symmetry\n",
    "\n",
    "  j) fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "\n",
    "* Missing attribute values: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import and read the breast-cancer.data.csv.\n",
    "df = pd.read_csv(\"../Resources/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key challenge against its detection is how to classify tumors into malignant (cancerous) or benign(non-cancerous). We ask you to complete the analysis of classifying these tumors using machine learning (with SVMs) and the Breast Cancer Wisconsin (Diagnostic) Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find null values\n",
    "for column in df.columns:\n",
    "    print(f\"Column {column} has {df[column].isnull().sum()} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate entries\n",
    "print(f\"Duplicate entries: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cutoff value and create a list of diagnosis to be replaced\n",
    "# use the variable name `diagnosis_to_replace`\n",
    "\n",
    "# Transform diagnosis\n",
    "def diagnosis_to_replace(diagnosis):\n",
    "    if diagnosis == \"M\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].apply(diagnosis_to_replace)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays also drop the id as that is not useful\n",
    "X = df.drop([\"diagnosis\", \"id\"], axis='columns')\n",
    "y = df[\"diagnosis\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick look at the features to see if the data is distributed normally or not\n",
    "X.hist(figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant number of features that have a strong right skew, so for some models the data will need to be transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we choose to look at our data from an outcome point of view to see if we can get any insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign = df.loc[df['diagnosis']==0]\n",
    "benign.drop(columns=['id','diagnosis'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malign = df.loc[df['diagnosis']==1]\n",
    "malign.drop(columns=['id','diagnosis'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# have a look at how the malign data looks compared to the benign in a few categories\n",
    "plt.hist(benign['radius_mean'], alpha=.5, label='B')\n",
    "plt.hist(malign['radius_mean'], alpha=.5, label='M')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(benign['texture_mean'], alpha=.5, label='B')\n",
    "plt.hist(malign['texture_mean'], alpha=.5, label='M')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(benign['perimeter_mean'], alpha=.5, label='B')\n",
    "plt.hist(malign['perimeter_mean'], alpha=.5, label='M')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(benign['smoothness_mean'], alpha=.5, label='B')\n",
    "plt.hist(malign['smoothness_mean'], alpha=.5, label='M')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can come back to this when we want to tweak the features for improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any negative values in df\n",
    "(df < 0).values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the label classes are not balanced the model will have a slight bias towards detecting benign results. This can be corrected by randomly removing some of the benign results so the numbers are equal. This should help reduce the number of false negatives. The only drawback is that the more rows we remove, the less overall training the model get and thus possibly worse performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:  Every time the next cell is run we will get different performance based on the randomness of the data that gets removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create smaller datasets with a different bias\n",
    "\n",
    "# balanced classes for no training bias\n",
    "balanced_df = df.drop(   df.loc[df['diagnosis'] == 0].sample(n=(benign.shape[0] - malign.shape[0])).index ).reset_index(drop=True)\n",
    "\n",
    "# slight bias towards malign\n",
    "bias = 50\n",
    "nb_df = df.drop(   df.loc[df['diagnosis'] == 0].sample(n=(benign.shape[0] - malign.shape[0] + bias)).index ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create features and labels based on these datasets with different bias\n",
    "b_X = balanced_df.drop([\"diagnosis\", \"id\"], axis='columns')\n",
    "b_y = balanced_df['diagnosis']\n",
    "\n",
    "nb_X = balanced_df.drop([\"diagnosis\", \"id\"], axis='columns')\n",
    "nb_y = balanced_df['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the different datasets: normal (positive bias), balanced and negative bias (after some exploration we found the results were best with a 31% test size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=74, stratify=y, test_size=0.31)\n",
    "\n",
    "b_X_train, b_X_test, b_y_train, b_y_test = train_test_split(b_X, b_y, random_state=74, stratify=b_y, test_size=0.31)\n",
    "\n",
    "nb_X_train, nb_X_test, nb_y_train, nb_y_test = train_test_split(nb_X, nb_y, random_state=74, stratify=nb_y, test_size=0.31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler instance since all values are positive to try for better results\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the MinMaxScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "b_X_scaler = scaler.fit(b_X_train)\n",
    "nb_X_scaler = scaler.fit(nb_X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "b_X_train_scaled = b_X_scaler.transform(b_X_train)\n",
    "b_X_test_scaled = b_X_scaler.transform(b_X_test)\n",
    "\n",
    "nb_X_train_scaled = nb_X_scaler.transform(nb_X_train)\n",
    "nb_X_test_scaled = nb_X_scaler.transform(nb_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In all cases it should be noted that we got different results each time we ran the code, even with the same parameters and seeds. Without doing exhaustive testing to find the mean and variance of the performance of the models it may be hard to prove which parameters gve the best fit. As that is likely the case we will consider results that are within a say 2% to be roughly equivelant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state = 1, max_iter=10000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "b_lr = LogisticRegression(random_state = 1, max_iter=10000)\n",
    "b_lr.fit(b_X_train_scaled, b_y_train)\n",
    "\n",
    "nb_lr = LogisticRegression(random_state = 1, max_iter=10000)\n",
    "nb_lr.fit(nb_X_train_scaled, nb_y_train)\n",
    "\n",
    "\n",
    "print(f\"Normal Training Data Score: {lr.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Normal Testing Data Score: {lr.score(X_test_scaled, y_test)}/n\")\n",
    "\n",
    "print(f\"Balanced Training Data Score: {lr.score(b_X_train_scaled, b_y_train)}\")\n",
    "print(f\"Balanced Testing Data Score: {lr.score(b_X_test_scaled, b_y_test)}/n\")\n",
    "\n",
    "print(f\"Bias Training Data Score: {lr.score(nb_X_train_scaled, nb_y_train)}\")\n",
    "print(f\"Bias Testing Data Score: {lr.score(nb_X_test_scaled, nb_y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we are really interested in is the number of false negatives as that is the worse way to fail. As such we can look at the confusion matrix and see what percent of results are false negatives. One thing that was discovered is that the smaller dataframes are more susceptible to variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_y_pred = lr.predict(X_test_scaled)\n",
    "b_lr_y_pred = b_lr.predict(X_test_scaled)\n",
    "nb_lr_y_pred = nb_lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cm = confusion_matrix(y_test, y_pred)\n",
    "lr_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the method is the same we will just pick the normal dataset to see how the results are biased\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see the important false negative rate\n",
    "fnr = fn/(tp+fn)\n",
    "print(f'False negative rate: {fnr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression we found the results to be very similar for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=74, n_estimators=200).fit(X_train_scaled, y_train)\n",
    "b_rfc = RandomForestClassifier(random_state=74, n_estimators=200).fit(b_X_train_scaled, b_y_train)\n",
    "nb_rfc = RandomForestClassifier(random_state=74, n_estimators=200).fit(nb_X_train_scaled, nb_y_train)\n",
    "\n",
    "print(f'Training Score: {rfc.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {rfc.score(X_test_scaled, y_test)}\\n')\n",
    "\n",
    "print(f'Balanced Training Score: {b_rfc.score(b_X_train_scaled, b_y_train)}')\n",
    "print(f'Balanced Testing Score: {b_rfc.score(b_X_test_scaled, b_y_test)}\\n')\n",
    "\n",
    "print(f'Bias Training Score: {nb_rfc.score(nb_X_train_scaled, nb_y_train)}')\n",
    "print(f'Bias Testing Score: {nb_rfc.score(nb_X_test_scaled, nb_y_test)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(X_test_scaled)\n",
    "b_y_pred = b_rfc.predict(b_X_test_scaled)\n",
    "nb_y_pred = nb_rfc.predict(nb_X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "b_cm = confusion_matrix(b_y_test, b_y_pred)\n",
    "btn, bfp, bfn, btp = confusion_matrix(b_y_test, b_y_pred).ravel()\n",
    "\n",
    "nb_cm = confusion_matrix(nb_y_test, nb_y_pred)\n",
    "nbtn, nbfp, nbfn, nbtp = confusion_matrix(nb_y_test, nb_y_pred).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time we will look at all the biases between all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "tpr = tp/(tp+fn)\n",
    "btpr = btp/(btp+bfn)\n",
    "nbtpr = nbtp/(nbtp+nbfn)\n",
    "\n",
    "# Specificity or true negative rate\n",
    "tnr = tn/(tn+fp) \n",
    "btnr = btn/(btn+bfp) \n",
    "nbtnr = nbtn/(nbtn+nbfp)\n",
    "\n",
    "# Precision or positive predictive value\n",
    "ppv = tp/(tp+fp)\n",
    "bppv = btp/(btp+bfp)\n",
    "nbppv = nbtp/(nbtp+nbfp)\n",
    "\n",
    "# Negative predictive value\n",
    "npv = tn/(tn+fn)\n",
    "bnpv = btn/(btn+bfn)\n",
    "nbnpv = nbtn/(nbtn+nbfn)\n",
    "\n",
    "# Fall out or false positive rate\n",
    "fpr = fp/(fp+tn)\n",
    "bfpr = bfp/(bfp+btn)\n",
    "nbfpr = nbfp/(nbfp+nbtn)\n",
    "\n",
    "# False negative rate\n",
    "fnr = fn/(tp+fn)\n",
    "bfnr = bfn/(btp+bfn)\n",
    "nbfnr = nbfn/(nbtp+nbfn)\n",
    "\n",
    "# False discovery rate\n",
    "fdr = fp/(tp+fp)\n",
    "bfdr = bfp/(btp+bfp)\n",
    "nbfdr = nbfp/(nbtp+nbfp)\n",
    "\n",
    "# Overall accuracy\n",
    "acc = (tp+tn)/(tp+fp+fn+tn)\n",
    "bacc = (btp+btn)/(btp+bfp+bfn+btn)\n",
    "nbacc = (nbtp+nbtn)/(nbtp+nbfp+nbfn+nbtn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'True positive rates: {tpr:.4f} b: {btpr:.4f} nb: {nbtpr:.4f}')\n",
    "print(f'True negative rates: {tnr:.4f} b: {btnr:.4f} nb: {nbtnr:.4f}')\n",
    "print(f'Positive predictive values: {ppv:.4f} b: {bppv:.4f} nb: {nbppv:.4f}')\n",
    "print(f'Negative predictive values: {npv:.4f} b: {bnpv:.4f} nb: {nbnpv:.4f}')\n",
    "print(f'False positive rates: {fpr:.4f} b: {bfpr:.4f} nb: {nbfpr:.4f}')\n",
    "print(f'False negative rates: {fnr:.4f} b: {bfnr:.4f} nb: {nbfnr:.4f}')\n",
    "print(f'False discovery rates: {fdr:.4f} b: {bfdr:.4f} nb: {nbfdr:.4f}')\n",
    "print(f'Overall Accuracies: {acc:.4f} b: {bacc:.4f} nb: {nbacc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different k values to find which has the highest accuracy\n",
    "train_scores = []\n",
    "btrain_scores = []\n",
    "nbtrain_scores = []\n",
    "test_scores = []\n",
    "btest_scores = []\n",
    "nbtest_scores = []\n",
    "\n",
    "\n",
    "for k in range(1, 20, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    bknn = KNeighborsClassifier(n_neighbors=k)\n",
    "    nbknn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    bknn.fit(b_X_train_scaled, b_y_train)\n",
    "    nbknn.fit(nb_X_train_scaled, nb_y_train)\n",
    "\n",
    "    train_score = knn.score(X_train_scaled, y_train)\n",
    "    btrain_score = bknn.score(b_X_train_scaled, b_y_train)\n",
    "    nbtrain_score = nbknn.score(nb_X_train_scaled, nb_y_train)\n",
    "\n",
    "    test_score = knn.score(X_test_scaled, y_test)\n",
    "    btest_score = bknn.score(b_X_test_scaled, b_y_test)\n",
    "    nbtest_score = nbknn.score(nb_X_test_scaled, nb_y_test)\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    btrain_scores.append(btrain_score)\n",
    "    nbtrain_scores.append(nbtrain_score)\n",
    "\n",
    "    test_scores.append(test_score)\n",
    "    btest_scores.append(btest_score)\n",
    "    nbtest_scores.append(nbtest_score)\n",
    "\n",
    "\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.4f}/{test_score:.4f}\")\n",
    "    print(f\"k: {k}, bTrain/bTest Score: {btrain_score:.4f}/{btest_score:.4f}\")\n",
    "    print(f\"k: {k}, nbTrain/nbTest Score: {nbtrain_score:.4f}/{nbtest_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like k = 3 is best with the normal (larger dataset) \n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('k=3 Test Acc: %.4f' % knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "cm_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktn, kfp, kfn, ktp = confusion_matrix(y_test, y_pred_knn).ravel()\n",
    "\n",
    "kfnr = kfn/(ktp+kfn)\n",
    "\n",
    "print(f'False negative rate: {kfnr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that knn performed as well as the balanced random forest in both accuracy and false negative rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C-Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "lsvc = SVC(kernel='linear')\n",
    "psvc2 = SVC(kernel='poly', degree=2)\n",
    "psvc3 = SVC(kernel='poly', degree=3)\n",
    "psvc8 = SVC(kernel='poly', degree=8)\n",
    "psvc9 = SVC(kernel='poly', degree=9)\n",
    "gsvc = SVC(kernel='rbf')\n",
    "ssvc = SVC(kernel='sigmoid')\n",
    "lsvc.fit(X_train, y_train)\n",
    "psvc2.fit(X_train, y_train)\n",
    "psvc3.fit(X_train, y_train)\n",
    "psvc8.fit(X_train, y_train)\n",
    "psvc9.fit(X_train, y_train)\n",
    "gsvc.fit(X_train, y_train)\n",
    "ssvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_l = lsvc.predict(X_test)\n",
    "y_pred_p2 = psvc2.predict(X_test)\n",
    "y_pred_p3 = psvc3.predict(X_test)\n",
    "y_pred_p8 = psvc8.predict(X_test)\n",
    "y_pred_p9 = psvc9.predict(X_test)\n",
    "y_pred_g = gsvc.predict(X_test)\n",
    "y_pred_s = ssvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a quicker summary (though without the false negative rate) we can use a classification report.\n",
    "# This can save time in deciding if a model is worth further investigation.\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_l))\n",
    "print(classification_report(y_test, y_pred_p2))\n",
    "print(classification_report(y_test, y_pred_p3))\n",
    "print(classification_report(y_test, y_pred_p8))\n",
    "print(classification_report(y_test, y_pred_p9))\n",
    "print(classification_report(y_test, y_pred_g))\n",
    "print(classification_report(y_test, y_pred_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction and unsupervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we reduce the dimensionality (we will first try cutting it in half)\n",
    "pca = PCA(n_components=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale = scaler.fit(X)\n",
    "X_scaled = X_scale.transform(X)\n",
    "pca_X = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to look at a few parameters of t-sne to optimize since it can be sensitive to perplexity based on data density - we also only want to see 2 dimensions\n",
    "tsne = TSNE(n_components=2, perplexity=15, n_iter=1000, learning_rate=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_features = tsne.fit_transform(pca_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_features[:,0],tsne_features[:,1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now see how well kmeans can tell the data apart.  Since it is a binary test we can only have 2 clusters though.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2, random_state=74)\n",
    "km.fit(pca_X)\n",
    "cluster_pred = km.predict(pca_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_features[:,0], tsne_features[:,1], c=cluster_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "\n",
    "for i in pca.explained_variance_ratio_:\n",
    "  sum += i\n",
    "\n",
    "# here we can see that the 15 variables account for almost 99% of the variance\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y, cluster_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Keras-Tuner to find best parameters to optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can create a function to return our false negatives for use as a metric (https://scorrea92.medium.com/useful-metrics-functions-for-keras-and-tensorflow-b82af9b22c9e)\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def fn(y_true, y_pred):\n",
    "    return K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options:  method based off of class activities\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow keras-tuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','selu'])\n",
    "    \n",
    "    # Allow keras-tuner to decide number of neurons in first layer\n",
    "    # Input dimensions set to X_train_scaled.shape[1] to be automatically set to the number of features(columns)\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=80,\n",
    "        step=5), activation=activation, input_dim=X_train_scaled.shape[1]))\n",
    "\n",
    "    # Allow keras-tuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 5)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=80,\n",
    "            step=10),\n",
    "            activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy', fn])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the keras-tuner library\n",
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    overwrite=True,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=100,\n",
    "    hyperband_iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning:  This will take more than 15 minutes until I setup an early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the keras-tuner search for best hyperparameters\n",
    "tuner.search(X_train_scaled,y_train,epochs=10,validation_data=(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best hyperparameters from the search to put into the model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using the parameters from the tuner\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "# Check the structure of the model\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = best_model.fit(X_train_scaled, y_train, epochs=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy, fn = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy*100} %, False Negative Rate: {fn} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We were able to get the best results with the neural network by getting the highest accuracy and lowest false negative rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got the tuner to say 100% accuracy, but when I ran it, it only got 98.7% and the fn rate was 0.35%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "37e7fb2da5e50466637845802289cd9557b44701aaa67a0ddeb3b500da7113d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
